# -*- coding: utf-8 -*-
"""house_prices.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cwG5BWugtPew-ipQozHzudNLYxP1kieh
"""

#Código de Ally_MV 
#código de predicción de los precios de viviendas 
#Importamos el dataset 
from google.colab import files
uploaded = files.upload()

import pandas as pd 
import numpy 
#Cargaremos y los datos 
df = pd.read_csv('housepricedata.csv')

#leeremos
df

#Se convierten los datos a una matriz para manejarlos mejor
dataset = df.values

dataset

X = dataset[:,0:10] #carácteristicas de entrada 
Y = dataset[:,10] #carácteristicas a predecir

from sklearn import preprocessing #vamos a utilizar el código de prepocesamiento

#se escala el conjunto de datos para que las carácteristicas de entradas se mantengan entre 0 y 1
min_max_scaler = preprocessing.MinMaxScaler() 
X_scale = min_max_scaler.fit_transform(X)
#el conjuto de datos a escala ya está guardado la línea de abajo lo muestra
X_scale

#se han tomado los datos entre 0 y 1 con el fin de poder ayudar al entrenamiento de nuestra red

#se divide el conjunto de datos en prueba y entrenamiento 
from sklearn.model_selection import train_test_split
#dividimos el set de datos
#set de datos entrenado (validación)
X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.3)  #30% de datos a evaluar
#set de datos de prueba 
X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)
#mostramos las dimensiones
print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)

#manos a la obra, es momento de crear nuestra red neuronal secuencial

#se importan las librerías 
from keras.models import  Sequential 
from keras.layers import Dense

# explicación a detalles 
# model =Sequential([  ]) se almacena nuestro modelo en una variable y lo describimos secuencialmente capa a capa

model = Sequential([
      Dense(32, activation ='relu', input_shape=(10,)), #primer capa capa con 32 neuronas 
      Dense(32, activation='relu'),                     #se activa la función relu
      Dense(1, activation='sigmoid'),                   #una capa densa de una sóla neurona con la función sigmoidea
])

model.compile(optimizer='sgd',  #calculamos el descenso del gradiente 
              loss='binary_crossentropy',  #función de pérdida entropía cruzada binaria
              metrics=['accuracy'])         # rastreo de precisión

#ajustaremos los parámetros a los datos 

history= model.fit(X_train, Y_train,  
         batch_size=32, epochs=100, #se especifica el tamaño del mini-lote y el tiempo de entrenamiento
          validation_data=(X_val, Y_val))

#Ya tenemos nuestro modelo entrenado!!

#evaluaremos mediante un conjunto de prubeas
model.evaluate(X_test, Y_test)[1]

#hasta este momento nuestra red se encuentra entrenada

#A continuación vamos a visualizar las pérdidas y la precisión, para poder lograrlo se trazan gráficos

#Se importan las funciones a utilizar
import matplotlib.pyplot as plt 
from keras.utils.vis_utils import plot_model
from keras.callbacks import History

#Visualizamos
plt.plot(history.history['loss']) #trazo de la pérdida
plt.plot(history.history['val_loss']) #trazo del valor
plt.title('Model loss') #título del gráfico
plt.ylabel('Loss') # etiqueta sobre eje Y
plt.xlabel('Epoch') #etiqueta sobre eje X
plt.legend(['Train', 'Val'], loc='upper right') # ubicación de la leyenda 
plt.show() #muestra el gráfico

# A continuación vamos a trazar precisión de entrenamiento y precisión de validación

plt.plot(history.history['accuracy']) 
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='lower right')
plt.show()